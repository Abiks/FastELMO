{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'sru[cuda]<2.1.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sru import SRU, SRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
    "print(\"Number of Tagged Sentences \",len(tagged_sentence))\n",
    "tagged_words=[tup for sent in tagged_sentence for tup in sent]\n",
    "print(\"Total Number of Tagged words\", len(tagged_words))\n",
    "vocab=set([word for word,tag in tagged_words])\n",
    "print(\"Vocabulary of the Corpus\",len(vocab))\n",
    "tags=set([tag for word,tag in tagged_words])\n",
    "print(\"Number of Tags in the Corpus \",len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(tagged_sentence,test_size=0.2,random_state=1234)\n",
    "print(\"Number of Sentences in Training Data \",len(train_set))\n",
    "print(\"Number of Sentences in Testing Data \",len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {}\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "assert len(tag_to_ix) == len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(Tagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.sru = rnn\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size) # -1 to not count padding tag\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        sru_out, _ = self.sru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(sru_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sru_module = SRU(EMBEDDING_DIM, HIDDEN_DIM,\n",
    "                          num_layers = 2,          # number of stacking RNN layers\n",
    "                          dropout = 0.0,           # dropout applied between RNN layers\n",
    "                          bidirectional = False,   # bidirectional RNN\n",
    "                          layer_norm = False,      # apply layer normalization on the output of each layer\n",
    "                          highway_bias = 0,        # initial bias of highway gate (<= 0)\n",
    "                          rescale = True,          # whether to use scaling correction\n",
    "                        )\n",
    "\n",
    "lstm_module = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(model, dataset, scorer = accuracy_score):\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    sum = 0\n",
    "    for line in dataset:\n",
    "      d = dict(line)   \n",
    "      sentence = d.keys()\n",
    "      tags = d.values()\n",
    "\n",
    "      targets = prepare_sequence(tags, tag_to_ix).to(device)\n",
    "      inputs = prepare_sequence(sentence, word_to_ix).to(device)\n",
    "      \n",
    "      tag_scores = model(inputs)\n",
    "      y_pred = tag_scores.argmax(axis=1)\n",
    "      sum += scorer(targets.cpu(), y_pred.cpu())\n",
    "\n",
    "    return sum/len(dataset)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "def train(model, loss_function = nn.NLLLoss(), optimizer = optim.SGD,  n_epoch = 3):\n",
    "    warnings.filterwarnings(action='once')\n",
    "    start = time.time()\n",
    "    \n",
    "    optimizer = optimizer(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        for line in train_set:\n",
    "            d = dict(line)\n",
    "            sentence = d.keys()\n",
    "            tags = d.values()\n",
    "\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix).to(device)\n",
    "            targets = prepare_sequence(tags, tag_to_ix).to(device)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model.forward(sentence=sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch}     Acc_train: {calculate_score(model, train_set)},  Acc_test: {calculate_score(model, test_set)}\")    \n",
    "\n",
    "    print(\"Elapsed time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sru = Tagger(sru_module, EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix)).to(device)\n",
    "lstm = Tagger( lstm_module, EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(sru, n_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(lstm, n_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дальше идёт пока не законченная часть, где будет реализована загрузка через генератор батчами "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, data):\n",
    "        'Initialization'\n",
    "        self.list_data = data\n",
    "        \n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        line = self.list_data[index]\n",
    "\n",
    "        d = dict(line)       \n",
    "        words = d.keys()\n",
    "        tags = d.values()\n",
    "        sentence_in = prepare_sequence(words, word_to_ix)#.to(device)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)#.to(device)\n",
    "        \n",
    "        return sentence_in, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = MyDataset(train_set)\n",
    "testing_set = MyDataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import rnn\n",
    "\n",
    "class PadSequence:\n",
    "    def __call__(self, batch):\n",
    "#         max_length = max(map(len, lines))\n",
    "    \n",
    "#         sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "        sequences = [x[0] for x in batch]\n",
    "        tags = [x[1] for x in batch]\n",
    "\n",
    "        sequences_padded = rnn.pad_sequence(sequences, batch_first = True, padding_value=-1)\n",
    "        tags_padded = rnn.pad_sequence(tags, batch_first = True, padding_value=-1)\n",
    "        # Also need to store the length of each sequence\n",
    "        # This is later needed in order to unpad the sequences\n",
    "#         lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "        # Don't forget to grab the labels of the *sorted* batch\n",
    "        \n",
    "#         new_batch = [(sequences_padded[i], batch[i][1]) for i in range(len(batch))]\n",
    "        \n",
    "        return sequences_padded, tags_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 3,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6,\n",
    "         'collate_fn':PadSequence()}\n",
    "\n",
    "train_loader = data.DataLoader(training_set, **params)\n",
    "\n",
    "test_loader = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(enumerate(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id, (sentences, tags) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "def train_generator(model, train_loader, test_loader=None, loss_function = nn.NLLLoss(), n_epoch = 3):\n",
    "    warnings.filterwarnings(action='once')\n",
    "    start = time.time()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        for batch_id, (sentences, tags) in enumerate(train_loader):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = sentences.to(device)\n",
    "            targets = tags.to(device)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model.forward(sentence=sentence_in)\n",
    "            \n",
    "            print(tag_scores.shape)\n",
    "            print(targets.shape)\n",
    "\n",
    "            assert False\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch: {epoch}/{n_epoch}\")\n",
    "#         print(f\"Epoch: {epoch}     Acc_train: {calculate_score(model, train_set)},  Acc_test: {calculate_score(model, test_set)}\")    \n",
    "\n",
    "    print(\"Elapsed time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator(sru, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "model = sru\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "loaders = {\"train\": train_loader, \"valid\": test_loader}\n",
    "logdir = \"./logs/sru\"\n",
    "\n",
    "# model runner\n",
    "runner = SupervisedRunner()\n",
    "\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
