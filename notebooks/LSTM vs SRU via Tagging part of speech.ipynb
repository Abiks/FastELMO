{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'sru[cuda]<2.1.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sru import SRU, SRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('treebank')\n",
    "# nltk.download('universal_tagset')\n",
    "\n",
    "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
    "print(\"Number of Tagged Sentences \",len(tagged_sentence))\n",
    "tagged_words=[tup for sent in tagged_sentence for tup in sent]\n",
    "print(\"Total Number of Tagged words\", len(tagged_words))\n",
    "vocab=set([word for word,tag in tagged_words])\n",
    "print(\"Vocabulary of the Corpus\",len(vocab))\n",
    "tags=set([tag for word,tag in tagged_words])\n",
    "print(\"Number of Tags in the Corpus \",len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(tagged_sentence,test_size=0.2,random_state=1234)\n",
    "print(\"Number of Sentences in Training Data \",len(train_set))\n",
    "print(\"Number of Sentences in Testing Data \",len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix) + 1\n",
    "\n",
    "tag_to_ix = {}\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix) + 1\n",
    "\n",
    "assert len(tag_to_ix) == len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Tagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, use_lstm, vocab_size, tagset_size):\n",
    "        super(Tagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tagset_size = tagset_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.use_lstm = use_lstm\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        if use_lstm:\n",
    "            self.reccurent_layer = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,)\n",
    "        else:\n",
    "            self.reccurent_layer = SRU(embedding_dim, hidden_dim,\n",
    "                                      num_layers = 2,          # number of stacking RNN layers\n",
    "                                      dropout = 0.0,           # dropout applied between RNN layers\n",
    "                                      bidirectional = False,   # bidirectional RNN\n",
    "                                      layer_norm = False,      # apply layer normalization on the output of each layer\n",
    "                                      highway_bias = 0,        # initial bias of highway gate (<= 0)\n",
    "                                      rescale = True,          # whether to use scaling correction\n",
    "                                    )\n",
    "            \n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "    \n",
    "    def forward(self, sentence, lengths):\n",
    "        embeds = self.word_embeddings(sentence) # ---> [B x L x D]\n",
    "        \n",
    "        if self.use_lstm:\n",
    "            packed_input = pack_padded_sequence(embeds, lengths=lengths, enforce_sorted=False, batch_first=False)\n",
    "            packed_out, _ = self.reccurent_layer(packed_input) # ---> [L x B x D]\n",
    "            output, input_sizes = pad_packed_sequence(packed_out, batch_first=False)\n",
    "\n",
    "        else:\n",
    "            mask =  rnn.pad_sequence([torch.zeros(l) for l in lengths], batch_first = False, padding_value=1).to(device)\n",
    "            # assumed in mini-batch length-first\n",
    "            assert mask.shape == sentence.shape, \"Wrong shape, should be [L x B]\"\n",
    "\n",
    "            output, _ = self.reccurent_layer(embeds, mask_pad = mask)              \n",
    "        \n",
    "        tag_space = self.hidden2tag(output) \n",
    "        tag_scores = F.log_softmax(tag_space, dim=2)\n",
    "\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data):\n",
    "        'Initialization'\n",
    "        self.list_data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        line = self.list_data[index]\n",
    "\n",
    "        d = dict(line)       \n",
    "        words = d.keys()\n",
    "        tags = d.values()\n",
    "        sentence_in = prepare_sequence(words, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        return sentence_in, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = MyDataset(train_set)\n",
    "testing_set = MyDataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import rnn\n",
    "\n",
    "class PadSequence:\n",
    "    def __call__(self, batch):\n",
    "        sequences = [x[0] for x in batch]\n",
    "        tags = [x[1] for x in batch]\n",
    "\n",
    "        sequences_padded = rnn.pad_sequence(sequences, batch_first = False, padding_value=0)\n",
    "        tags_padded = rnn.pad_sequence(tags, batch_first = False, padding_value=0)\n",
    "        \n",
    "        # Also need to store the length of each sequence\n",
    "        # This is later needed in order to unpad the sequences\n",
    "        lengths = torch.LongTensor(list(map(len, sequences)))\n",
    "        \n",
    "        return sequences_padded, tags_padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score_generator(model, dataset):\n",
    "\n",
    "    params = {'batch_size': 512,\n",
    "          'shuffle': False,\n",
    "         'collate_fn':PadSequence()}\n",
    "\n",
    "    loader = data.DataLoader(dataset, **params)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sum = 0\n",
    "        n_missmatch = 0\n",
    "        n_all = 0\n",
    "        for batch_id, (sentences, tags, lengths) in enumerate(loader):\n",
    "            sentences = sentences.to(device)\n",
    "            tag_scores = model.forward(sentences, lengths)\n",
    "            y_pred = tag_scores.argmax(axis=2)\n",
    "\n",
    "            preds = pack_padded_sequence(y_pred, lengths=lengths, enforce_sorted=False, batch_first=False)\n",
    "            targets = pack_padded_sequence(tags, lengths=lengths, enforce_sorted=False, batch_first=False)\n",
    "\n",
    "            n_missmatch += (targets.data != preds.data.cpu()).sum()\n",
    "            n_all += len(targets.data)\n",
    "    \n",
    "    return 1 - n_missmatch.numpy()/n_all  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "def train_generator(model, train_loader, eval_datasets=None, loss_function = nn.NLLLoss(), n_epoch = 3):\n",
    "    warnings.filterwarnings(action='once')\n",
    "    start = time.time()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        for batch_id, (sentences, tags, lengths) in enumerate(train_loader):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = sentences.to(device)\n",
    "            targets = tags.to(device)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model.forward(sentence_in, lengths)\n",
    "            \n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "\n",
    "            loss = loss_function(tag_scores.view(-1, model.tagset_size), targets.view(-1))\n",
    "            loss.backward()            \n",
    "            optimizer.step()\n",
    "            \n",
    "        if eval_datasets is None:\n",
    "            print(f\"Epoch: {epoch + 1}/{n_epoch}\")\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch + 1}/{n_epoch}     Acc_eval_0: {calculate_score_generator(model, eval_datasets[0])},  Acc_eval_1: {calculate_score_generator(model, eval_datasets[1])}\")    \n",
    "\n",
    "    print(\"Elapsed time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 8,\n",
    "          'shuffle': True,\n",
    "         'collate_fn':PadSequence()}\n",
    "\n",
    "train_loader = data.DataLoader(training_set, **params)\n",
    "test_loader = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sru = Tagger(EMBEDDING_DIM, HIDDEN_DIM, use_lstm=False, vocab_size=len(word_to_ix) + 1, tagset_size=len(tag_to_ix) + 1).to(device)\n",
    "lstm = Tagger(EMBEDDING_DIM, HIDDEN_DIM, use_lstm=True, vocab_size=len(word_to_ix) + 1, tagset_size=len(tag_to_ix) + 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator(sru, train_loader, n_epoch=10, eval_datasets=(training_set, testing_set)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator(lstm, train_loader, n_epoch=10, eval_datasets=(training_set, testing_set)) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
