{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install 'sru[cuda]<2.1.9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sru import SRU, SRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tagged Sentences  3914\n",
      "Total Number of Tagged words 100676\n",
      "Vocabulary of the Corpus 12408\n",
      "Number of Tags in the Corpus  12\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tagged_sentence = nltk.corpus.treebank.tagged_sents(tagset='universal')\n",
    "print(\"Number of Tagged Sentences \",len(tagged_sentence))\n",
    "tagged_words=[tup for sent in tagged_sentence for tup in sent]\n",
    "print(\"Total Number of Tagged words\", len(tagged_words))\n",
    "vocab=set([word for word,tag in tagged_words])\n",
    "print(\"Vocabulary of the Corpus\",len(vocab))\n",
    "tags=set([tag for word,tag in tagged_words])\n",
    "print(\"Number of Tags in the Corpus \",len(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Sentences in Training Data  3131\n",
      "Number of Sentences in Testing Data  783\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(tagged_sentence,test_size=0.2,random_state=1234)\n",
    "print(\"Number of Sentences in Training Data \",len(train_set))\n",
    "print(\"Number of Sentences in Testing Data \",len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "word_to_ix = {}\n",
    "\n",
    "for word in vocab:\n",
    "    if word not in word_to_ix:\n",
    "        word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {}\n",
    "for tag in tags:\n",
    "    if tag not in tag_to_ix:\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "assert len(tag_to_ix) == len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(Tagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.sru = rnn\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size) # -1 to not count padding tag\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        sru_out, _ = self.sru(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(sru_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sru_module = SRU(EMBEDDING_DIM, HIDDEN_DIM,\n",
    "                          num_layers = 2,          # number of stacking RNN layers\n",
    "                          dropout = 0.0,           # dropout applied between RNN layers\n",
    "                          bidirectional = False,   # bidirectional RNN\n",
    "                          layer_norm = False,      # apply layer normalization on the output of each layer\n",
    "                          highway_bias = 0,        # initial bias of highway gate (<= 0)\n",
    "                          rescale = True,          # whether to use scaling correction\n",
    "                        )\n",
    "\n",
    "lstm_module = nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, num_layers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(model, dataset, scorer = accuracy_score):\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    sum = 0\n",
    "    for line in dataset:\n",
    "      d = dict(line)   \n",
    "      sentence = d.keys()\n",
    "      tags = d.values()\n",
    "\n",
    "      targets = prepare_sequence(tags, tag_to_ix).to(device)\n",
    "      inputs = prepare_sequence(sentence, word_to_ix).to(device)\n",
    "      \n",
    "      tag_scores = model(inputs)\n",
    "      y_pred = tag_scores.argmax(axis=1)\n",
    "      sum += scorer(targets.cpu(), y_pred.cpu())\n",
    "\n",
    "    return sum/len(dataset)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "def train(model, loss_function = nn.NLLLoss(), optimizer = optim.SGD,  n_epoch = 3):\n",
    "    warnings.filterwarnings(action='once')\n",
    "    start = time.time()\n",
    "    \n",
    "    optimizer = optimizer(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        for line in train_set:\n",
    "            d = dict(line)        # это надо убрать в какой-нибудь даталоадер\n",
    "            sentence = d.keys()\n",
    "            tags = d.values()\n",
    "\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix).to(device)\n",
    "            targets = prepare_sequence(tags, tag_to_ix).to(device)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model.forward(sentence=sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch}     Acc_train: {calculate_score(model, train_set)},  Acc_test: {calculate_score(model, test_set)}\")    \n",
    "\n",
    "    print(\"Elapsed time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sru = Tagger(sru_module, EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix)).to(device)\n",
    "lstm = Tagger( lstm_module, EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/torch/csrc/autograd/python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Acc_train: 0.9134083972665801,  Acc_test: 0.8717780134319727\n",
      "Epoch: 1     Acc_train: 0.9573804354602311,  Acc_test: 0.8897232979056512\n",
      "Epoch: 2     Acc_train: 0.9758656068017106,  Acc_test: 0.8995575013109696\n",
      "Epoch: 3     Acc_train: 0.9866326241295851,  Acc_test: 0.9060705340588753\n",
      "Epoch: 4     Acc_train: 0.9918186801916291,  Acc_test: 0.9088534869039906\n",
      "Epoch: 5     Acc_train: 0.9957561515431828,  Acc_test: 0.9093400871456797\n",
      "Epoch: 6     Acc_train: 0.9971287358465315,  Acc_test: 0.9112057387567097\n",
      "Epoch: 7     Acc_train: 0.9984518165759537,  Acc_test: 0.9147247650698923\n",
      "Epoch: 8     Acc_train: 0.998668492839349,  Acc_test: 0.915627818146878\n",
      "Epoch: 9     Acc_train: 0.9987867921671787,  Acc_test: 0.9153728949561035\n",
      "Elapsed time:  58.180185317993164\n"
     ]
    }
   ],
   "source": [
    "train(sru, n_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0     Acc_train: 0.8704674722212549,  Acc_test: 0.8475579741142379\n",
      "Epoch: 1     Acc_train: 0.9299420555680269,  Acc_test: 0.8864265099331329\n",
      "Epoch: 2     Acc_train: 0.9585335842306272,  Acc_test: 0.8988487330081532\n",
      "Epoch: 3     Acc_train: 0.9726328193997663,  Acc_test: 0.9047405195299528\n",
      "Epoch: 4     Acc_train: 0.9828264538087873,  Acc_test: 0.9073311489434541\n",
      "Epoch: 5     Acc_train: 0.988841143521125,  Acc_test: 0.9057767908506523\n",
      "Epoch: 6     Acc_train: 0.9918790984642692,  Acc_test: 0.9087998466421776\n",
      "Epoch: 7     Acc_train: 0.994661851691341,  Acc_test: 0.9099361746200852\n",
      "Epoch: 8     Acc_train: 0.9965485282465362,  Acc_test: 0.9124675602534866\n",
      "Epoch: 9     Acc_train: 0.9972847208330112,  Acc_test: 0.9125284522705993\n",
      "Elapsed time:  101.28886771202087\n"
     ]
    }
   ],
   "source": [
    "train(lstm, n_epoch = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дальше идёт пока не законченная часть, где будет реализована загрузка через генератор батчами "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, data):\n",
    "        'Initialization'\n",
    "        self.list_data = data\n",
    "        \n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        line = self.list_data[index]\n",
    "\n",
    "        d = dict(line)       \n",
    "        words = d.keys()\n",
    "        tags = d.values()\n",
    "        sentence_in = prepare_sequence(words, word_to_ix)#.to(device)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)#.to(device)\n",
    "        \n",
    "        return sentence_in, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = MyDataset(train_set)\n",
    "testing_set = MyDataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import rnn\n",
    "\n",
    "class PadSequence:\n",
    "    def __call__(self, batch):\n",
    "#         max_length = max(map(len, lines))\n",
    "    \n",
    "#         sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
    "        sequences = [x[0] for x in batch]\n",
    "        tags = [x[1] for x in batch]\n",
    "\n",
    "        sequences_padded = rnn.pad_sequence(sequences, batch_first = True, padding_value=-1)\n",
    "        tags_padded = rnn.pad_sequence(tags, batch_first = True, padding_value=-1)\n",
    "        # Also need to store the length of each sequence\n",
    "        # This is later needed in order to unpad the sequences\n",
    "#         lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "        # Don't forget to grab the labels of the *sorted* batch\n",
    "        \n",
    "#         new_batch = [(sequences_padded[i], batch[i][1]) for i in range(len(batch))]\n",
    "        \n",
    "        return sequences_padded, tags_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 3,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6,\n",
    "         'collate_fn':PadSequence()}\n",
    "\n",
    "train_loader = data.DataLoader(training_set, **params)\n",
    "\n",
    "test_loader = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " (tensor([ 6889,  1164, 11590,  2456,  5513,   133, 11821, 11912,  7286,  7245,\n",
       "          11936,  6605,  3909,  7824,  9054,  2356,  3469,  9873,  3763,  9058,\n",
       "          10547, 11321,  8374]),\n",
       "  tensor([10, 11, 11, 11,  3, 11,  2, 10,  8, 11,  7,  0,  2,  4,  9,  1,  1,  6,\n",
       "           2, 10, 11,  0,  7])))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(enumerate(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id, (sentences, tags) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 3])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "\n",
    "def train_generator(model, train_loader, test_loader=None, loss_function = nn.NLLLoss(), n_epoch = 3):\n",
    "    warnings.filterwarnings(action='once')\n",
    "    start = time.time()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        for batch_id, (sentences, tags) in enumerate(train_loader):\n",
    "            # Step 1. Remember that Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = sentences.to(device)\n",
    "            targets = tags.to(device)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model.forward(sentence=sentence_in)\n",
    "            \n",
    "            print(tag_scores.shape)\n",
    "            print(targets.shape)\n",
    "\n",
    "            assert False\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            #  calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch: {epoch}/{n_epoch}\")\n",
    "#         print(f\"Epoch: {epoch}     Acc_train: {calculate_score(model, train_set)},  Acc_test: {calculate_score(model, test_set)}\")    \n",
    "\n",
    "    print(\"Elapsed time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-6f404d16e2f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-22ca44404b62>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(model, train_loader, test_loader, loss_function, n_epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# Step 1. Remember that Pytorch accumulates gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# We need to clear them out before each instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Step 2. Get our inputs ready for the network, that is, turn them into\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshare_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (710) : device-side assert triggered at /pytorch/aten/src/THC/generic/THCTensorMath.cu:26"
     ]
    }
   ],
   "source": [
    "train_generator(sru, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl import SupervisedRunner\n",
    "\n",
    "model = sru\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "loaders = {\"train\": train_loader, \"valid\": test_loader}\n",
    "logdir = \"./logs/sru\"\n",
    "\n",
    "# model runner\n",
    "runner = SupervisedRunner()\n",
    "\n",
    "# model training\n",
    "runner.train(\n",
    "    model=model,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    loaders=loaders,\n",
    "    logdir=logdir,\n",
    "    num_epochs=10,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
