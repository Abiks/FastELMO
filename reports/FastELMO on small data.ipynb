{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В книге о NLP на Pytorch предлагается делать следующие классы для загрузки данных:\n",
    "\n",
    "* Vocabulary: класс, который осуществляет преобразование слова в число. Сейчас этот класс будет просто оберткой над словарем. \n",
    "\n",
    "* Vectorizer: класс, осуществляющий преобразование текстовой строки в последовательность чисел. Также он гарантирует, что эти последовательности чисел будут иметь одну и ту же длину (сейчас это реализовано очень костыльно, в будущем нужно будет сделать гибкое изменение размера). Осуществляет часть работы \"подсчет частот встречаемости слов, выкидывание редких слов и создание Vocabulary\"\n",
    "\n",
    "* Dataset: это просто геттер, который возвращает одиночные семплы. \n",
    "\n",
    "* Dataloader: это обертка над Dataset, которая делает батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    # TODO: use DefaultDict instead of Dict\n",
    "    # TODO: use '[]' operators instead of add_token and lookup_token methods\n",
    "    # TODO: implement special tokens for numbers (<NUM> and maybe <YEAR>)\n",
    "    \n",
    "    def __init__(self, token_to_idx=None, \n",
    "                 add_unk=True, unk_token='<UNK>', \n",
    "                 add_start_end=True, start_token='<START>', end_token='<END>'):\n",
    "        \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = dict()\n",
    "            \n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx:token for token, idx in self._token_to_idx.items()}\n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self._add_start_end = add_start_end\n",
    "        self._start_token = start_token\n",
    "        self._end_token = end_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if self._add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "        \n",
    "        self._start_index = -1\n",
    "        self._end_index = -1\n",
    "        if self._add_start_end:\n",
    "            self._start_index = self.add_token(start_token)\n",
    "            self._end_index = self.add_token(end_token)\n",
    "            \n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Update mapping dicts basen on the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"\n",
    "        Retrieve the index associated with the token\n",
    "        or the UNK index if token isn't present\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"\n",
    "        Return the token associated with the index\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f'the index {index} is not in the Vocabulary')\n",
    "        else:\n",
    "            return self._idx_to_token[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split(text):\n",
    "    return re.findall(r'[\\w\\d]+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, train_vocab: Vocabulary, num_samples: int):\n",
    "        self.train_vocab = train_vocab\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    def vectorize(self, text: list):\n",
    "        \"\"\"\n",
    "        Create a numpy vector with indices of tokens in texts\n",
    "        \"\"\"\n",
    "        vectorized_texts = []\n",
    "        \n",
    "        max_text_len = 100 # TODO: don't use particular number\n",
    "        \n",
    "        vectorized_text = [self.train_vocab._start_index] # TODO: implement case without start and end tokens\n",
    "            \n",
    "        for word in split(text):\n",
    "            word_idx = self.train_vocab.lookup_token(word)\n",
    "            vectorized_text.append(word_idx)\n",
    "        \n",
    "        vectorized_text.append(self.train_vocab._end_index)\n",
    "        vectorized_text.extend([0] * (max_text_len - len(vectorized_text)))\n",
    "        \n",
    "        return vectorized_text\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def from_text_file(cls, path_to_file: Path, cutoff=1):\n",
    "        \"\"\"\n",
    "        Instantiate the vectorizer from text file\n",
    "        Words with frequency equal of less than cutoff won't be added in words dictionary        \n",
    "        \"\"\"\n",
    "        word_count = defaultdict(int)\n",
    "        num_samples = 0\n",
    "        \n",
    "        with path_to_file.open() as f:\n",
    "            for line in tqdm(f.readlines()):\n",
    "                num_samples += 1\n",
    "                for word in split(line):\n",
    "                    word_count[word] += 1\n",
    "        \n",
    "        cutted_word_dict = dict()\n",
    "        \n",
    "        for word in word_count.keys():\n",
    "            if word_count[word] > cutoff:\n",
    "                cutted_word_dict[word] = word_count[word]\n",
    "\n",
    "        \n",
    "        return cls(train_vocab=Vocabulary(cutted_word_dict), num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1516335/1516335 [00:12<00:00, 121108.09it/s]\n"
     ]
    }
   ],
   "source": [
    "path_to_data = Path('/Datasets/Wikipedia/data for small model/test.txt')\n",
    "\n",
    "vectorizer = Vectorizer.from_text_file(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import linecache\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastDataset(Dataset):\n",
    "    def __init__(self, filename, vectorizer):\n",
    "        self._filename = str(filename)\n",
    "        self._vectorizer = vectorizer\n",
    "        self._total_data = self.rawcount(filename)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        line = ' '.join(split(linecache.getline(self._filename, idx + 1)))\n",
    "        line_vectorized = self._vectorizer.vectorize(line)\n",
    "        \n",
    "        return {'raw_text': line,\n",
    "                'forward_target': np.array(line_vectorized[1:] + [0]),\n",
    "                'backward_target': np.array([0] + line_vectorized[:-1])\n",
    "               }\n",
    "      \n",
    "    def __len__(self):\n",
    "        return self._total_data\n",
    "    \n",
    "    \n",
    "    def rawcount(self, filename):\n",
    "        f = open(filename, 'rb')\n",
    "        lines = 0\n",
    "        buf_size = 1024 * 1024\n",
    "        read_f = f.raw.read\n",
    "\n",
    "        buf = read_f(buf_size)\n",
    "        while buf:\n",
    "            lines += buf.count(b'\\n')\n",
    "            buf = read_f(buf_size)\n",
    "\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FastDataset(path_to_data, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.1 µs ± 132 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_dataset[np.random.randint(100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john lindow says that the poem may describe a mix of the destruction of the race of giants and of humans as in ragnarök but that many of the predictions of disruption on earth could also fit the volcanic activity that is so common in iceland',\n",
       " 'the canadian shield also contains the mackenzie dike swarm which is the largest dike swarm known on earth',\n",
       " 'from 1930 until his death in 1953 abdulaziz ruled saudi arabia as an absolute monarchy',\n",
       " 'the u s sister city program began in 1956 when president dwight d eisenhower proposed a people to people citizen diplomacy initiative',\n",
       " 'a further development of this approach is programmable radio output processing where the parameters of the multiband compressor automatically change between different settings according to the current programme block style or the time of day',\n",
       " 'insignia is found on military hats or berets on the right and left shoulder on the uniform of all soldiers of the armed forces',\n",
       " 'jackson ritter and the satellite are taken to the secret underground wildfire laboratory a secure facility equipped with every known capacity for protection against microorganisms escaping into the environment',\n",
       " 'haman throws himself at her feet the king thinks that haman is attacking her and orders him to be put to death and gives all haman s possessions to esther',\n",
       " 'sainsbury donated 390 000 to progress and the movement for change between december 2011 and april 2013 while he was not on a uk electoral register which is contrary to electoral law leading to progress and the movement for change being fined by the electoral commission',\n",
       " 'for every 100 females age 18 and over there were 83 5 males',\n",
       " 'he wanted to pursue an independent course unbounded by racial restraints but became what he once despised',\n",
       " 'png includes a number of terrestrial ecoregions three new species of mammals were discovered in the forests of papua new guinea by an australian led expedition',\n",
       " 'in 2013 auctions were planned and for now any action by broadcasters is voluntary',\n",
       " 'burke was also entitled to wear the presidential unit citations presented to destroyer squadron 23 and to',\n",
       " 'his second four seasons resort at kōele in the mountains is currently being renovated',\n",
       " 'in 1983 hammer began composing film soundtracks with a synclavier adding a digital guitar interface in 1984']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['raw_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  15987,      21,    2245,  ...,       0,       0,       0],\n",
       "        [2504847,    4524,     779,  ...,       0,       0,       0],\n",
       "        [ 188454,    1490,   21188,  ...,       0,       0,       0],\n",
       "        ...,\n",
       "        [    386,  409126,   83114,  ...,       0,       0,       0],\n",
       "        [ 157105,   21741,   18940,  ...,       0,       0,       0],\n",
       "        [ 927700,    2866,     394,  ...,       0,       0,       0]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['forward_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[      0,  236888,   15987,  ...,       0,       0,       0],\n",
       "        [      0,  236888, 2504847,  ...,       0,       0,       0],\n",
       "        [      0,  236888,  188454,  ...,       0,       0,       0],\n",
       "        ...,\n",
       "        [      0,  236888,     386,  ...,       0,       0,       0],\n",
       "        [      0,  236888,  157105,  ...,       0,       0,       0],\n",
       "        [      0,  236888,  927700,  ...,       0,       0,       0]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['backward_target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот еще пример функции, которую будет полезно встроить в процесс обучения нашей модели:\n",
    "~~~\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (leo)",
   "language": "python",
   "name": "base_leo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
